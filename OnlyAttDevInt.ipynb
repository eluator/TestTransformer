{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49703b47-2b07-44a9-9d08-53836a5e6515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision, torchaudio\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from dataclasses import dataclass\n",
    "import einops\n",
    "from einops import einsum\n",
    "import math\n",
    "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "from easy_transformer import EasyTransformer\n",
    "import pandas as pd\n",
    "import json\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from bisect import bisect_right\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8408358-5210-475e-aff8-2cb3cb7f91b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eluator/AI_Safety/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1288 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>metadata</th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alsatian Cheese Tart\\n\\nFrench Chef Michel Ber...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'OpenWebText2']}</td>\n",
       "      <td>21</td>\n",
       "      <td>[tensor(50256), tensor(2348), tensor(49720), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depicted by four young winged men in Roman-lik...</td>\n",
       "      <td>{'pile_set_name': ['Wikipedia (en)', 'Pile-CC']}</td>\n",
       "      <td>81</td>\n",
       "      <td>[tensor(50256), tensor(10378), tensor(5722), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Available in:\\n\\ndescription\\n\\nOn Tuesday Mar...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'Pile-CC']}</td>\n",
       "      <td>102</td>\n",
       "      <td>[tensor(50256), tensor(10493), tensor(287), te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Date:\\n\\nDiscipline:\\n\\nSource:\\n\\nProduct num...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'Pile-CC']}</td>\n",
       "      <td>107</td>\n",
       "      <td>[tensor(50256), tensor(10430), tensor(25), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other meetings she had chaired. Stockholders w...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'Pile-CC']}</td>\n",
       "      <td>110</td>\n",
       "      <td>[tensor(50256), tensor(847), tensor(8292), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>wedding day and itinerary runs without a hitch...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'USPTO Backgroun...</td>\n",
       "      <td>34118665</td>\n",
       "      <td>[tensor(50256), tensor(86), tensor(6048), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>a faster motor. Motor speed is already a param...</td>\n",
       "      <td>{'pile_set_name': ['USPTO Backgrounds', 'Pile-...</td>\n",
       "      <td>34118691</td>\n",
       "      <td>[tensor(50256), tensor(64), tensor(5443), tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Flag Law on 29 May 1936.\\n\\nFlag of Turkey\\n\\n...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'OpenWebText2']}</td>\n",
       "      <td>34118794</td>\n",
       "      <td>[tensor(50256), tensor(34227), tensor(3854), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>president; he should, at best, be reviving cel...</td>\n",
       "      <td>{'pile_set_name': ['OpenWebText2', 'Pile-CC']}</td>\n",
       "      <td>34118819</td>\n",
       "      <td>[tensor(50256), tensor(22540), tensor(26), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>newly rebuilt temple after the return from the...</td>\n",
       "      <td>{'pile_set_name': ['Pile-CC', 'Pile-CC']}</td>\n",
       "      <td>34118866</td>\n",
       "      <td>[tensor(50256), tensor(3605), tensor(306), ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 contents  \\\n",
       "0       Alsatian Cheese Tart\\n\\nFrench Chef Michel Ber...   \n",
       "1       depicted by four young winged men in Roman-lik...   \n",
       "2       Available in:\\n\\ndescription\\n\\nOn Tuesday Mar...   \n",
       "3       Date:\\n\\nDiscipline:\\n\\nSource:\\n\\nProduct num...   \n",
       "4       other meetings she had chaired. Stockholders w...   \n",
       "...                                                   ...   \n",
       "999995  wedding day and itinerary runs without a hitch...   \n",
       "999996  a faster motor. Motor speed is already a param...   \n",
       "999997  Flag Law on 29 May 1936.\\n\\nFlag of Turkey\\n\\n...   \n",
       "999998  president; he should, at best, be reviving cel...   \n",
       "999999  newly rebuilt temple after the return from the...   \n",
       "\n",
       "                                                 metadata        id  \\\n",
       "0          {'pile_set_name': ['Pile-CC', 'OpenWebText2']}        21   \n",
       "1        {'pile_set_name': ['Wikipedia (en)', 'Pile-CC']}        81   \n",
       "2               {'pile_set_name': ['Pile-CC', 'Pile-CC']}       102   \n",
       "3               {'pile_set_name': ['Pile-CC', 'Pile-CC']}       107   \n",
       "4               {'pile_set_name': ['Pile-CC', 'Pile-CC']}       110   \n",
       "...                                                   ...       ...   \n",
       "999995  {'pile_set_name': ['Pile-CC', 'USPTO Backgroun...  34118665   \n",
       "999996  {'pile_set_name': ['USPTO Backgrounds', 'Pile-...  34118691   \n",
       "999997     {'pile_set_name': ['Pile-CC', 'OpenWebText2']}  34118794   \n",
       "999998     {'pile_set_name': ['OpenWebText2', 'Pile-CC']}  34118819   \n",
       "999999          {'pile_set_name': ['Pile-CC', 'Pile-CC']}  34118866   \n",
       "\n",
       "                                                   tokens  \n",
       "0       [tensor(50256), tensor(2348), tensor(49720), t...  \n",
       "1       [tensor(50256), tensor(10378), tensor(5722), t...  \n",
       "2       [tensor(50256), tensor(10493), tensor(287), te...  \n",
       "3       [tensor(50256), tensor(10430), tensor(25), ten...  \n",
       "4       [tensor(50256), tensor(847), tensor(8292), ten...  \n",
       "...                                                   ...  \n",
       "999995  [tensor(50256), tensor(86), tensor(6048), tens...  \n",
       "999996  [tensor(50256), tensor(64), tensor(5443), tens...  \n",
       "999997  [tensor(50256), tensor(34227), tensor(3854), t...  \n",
       "999998  [tensor(50256), tensor(22540), tensor(26), ten...  \n",
       "999999  [tensor(50256), tensor(3605), tensor(306), ten...  \n",
       "\n",
       "[1000000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "target_tokens = torch.cat((tokens[0][1:], torch.tensor([50256])), 0).view(1, -1)\n",
    "logits, cache = reference_gpt2.run_with_cache(reference_text)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])[-1]\n",
    "\n",
    "data = pd.read_json('../data/train_1M.jsonl', lines=True)\n",
    "data.insert(3, \"tokens\", [reference_gpt2.to_tokens(data.contents[i])[0] for i, x in enumerate(data.contents)], True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e0e387-bb5c-47af-9025-364f94677cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.tokens.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c68c82-f5f9-45a7-8f2a-03761eab32ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"depicted by four young winged men in Roman-like dresses, driving vessels and blowing air into horns. The central upper square is an old man representing the Year, with the Wheel of Time, while at the upper corners are the personifications of the Rivers of Paradise. The other six upper squares depict the Four Seasons, as well as Samson and Abel (or Cain).\\n\\nThe two lower corners show the personifications of the Sun (left, symbolizing Sunday) and the Moon (right, much deteriorated, symbolizing Monday), while the  side outer squares represent the months (only eight of which survive). At the bottom are incomplete scenes of the discovery of Holy Cross.\\n\\nSources\\n\\nExternal links\\n\\nOfficial cathedral's website \\nPage with details of the figures \\nPage with links to websites and the newest literature  (2012)Publication Date:\\n\\nDiscipline:\\n\\nSource:\\n\\nProduct number:\\n\\nLength:\\n\\nAlso Available in:\\n\\ndescription\\n\\nIn October 2004 FernÃ¡ndez Pujals, founder of Telepizza, an international home delivery pizza business, bought 24.9% of Jazztel (â‚¬90 million), a telecom company. At the time, Jazztel that was near bankruptcy and needed a capital injection to finish the year. Over the next ten years, FernÃ¡ndez Pujals led the restructuring of Jazztel's debt, reached an agreement with the former monopoly TelefÃ³nica, set up internal call centers, and transformed Jazztel into the fastest growing broadband operator in Spain. The case describes how FernÃ¡ndez Pujals designed and managed the board and led Jazztel towards profitable growth.\\n\\nlearning objective:\\n\\nThe case provides an example of board dynamics and corporate governance of a Spanish telecom company with a controlling shareholder serving as Chairman of the Board of Directors.\\n\\nPublication Date:\\n\\nDiscipline:\\n\\nSource:\\n\\nProduct number:\\n\\nLength:\\n\\nAlso\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.contents.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa748f8-0c8c-41fa-a14a-a2ea59b47b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_samples = len(self.data)\n",
    "        self.li = [len(tokens) for tokens in self.data.tokens.values]\n",
    "        self.cumulative_li = deepcopy(self.li)\n",
    "        self.cumulative_li[0] += 1 - self.sequence_length \n",
    "        for i in range(1, len(self.cumulative_li)):\n",
    "            self.cumulative_li[i] = self.cumulative_li[i-1] + self.cumulative_li[i] + 1 - self.sequence_length\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.cumulative_li[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        string_i = bisect_right(self.cumulative_li, idx)\n",
    "        diff = self.cumulative_li[string_i] - idx\n",
    "        context_window = self.data.tokens.values[string_i][-diff:self.sequence_length-diff]\n",
    "        if diff == 0:\n",
    "            target_window = torch.cat((self.data.tokens.values[string_i][-diff+1:self.sequence_length-diff], torch.tensor([50256])), 0).view(1, -1) \n",
    "        else:\n",
    "            target_window = self.data.tokens.values[string_i][-diff+1:self.sequence_length-diff+1]\n",
    "        return context_window, target_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e723565-b474-4494-848d-f4dd87d4ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomGPTDataset(data, sequence_length = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a135de0-59d7-430f-b5b8-4383272858ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 13375)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(training_data.li), max(training_data.li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "755a32a1-8c63-4ca2-906e-70e31cb8e8fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [0] at entry 0 and [128] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(training_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/AI_Safety/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [0] at entry 0 and [128] at entry 2"
     ]
    }
   ],
   "source": [
    "train_dataloader = utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "len(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fc275-02a6-4761-add0-295d2558d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.transformer = DemoTransformer(cfg)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        \n",
    "        logits = self.transformer(text)\n",
    "        loss = self.criterion(logits, target)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1930a28a-d031-470a-8f76-13ca1471bcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5890, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(logits.view(-1, 50257), target_tokens.flatten().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec41bbdb-5edc-4400-aef2-5c275f768135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    random_input = torch.randn(shape).cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    random_input = torch.randint(100, 1000, shape).cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str): \n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    reference_output = gpt2_layer(reference_input)\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcec88a-6312-4b13-8b29-c7ca2219def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb385aa2-1dd6-4038-a133-b0b3bd9e71d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "    \n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        if self.cfg.debug: print(\"Residual:\", residual.shape)\n",
    "        residual = residual - einops.reduce(residual, \"batch position d_model -> batch position 1\", \"mean\")\n",
    "        # Calculate the variance, square root it. Add in an epsilon to prevent divide by zero.\n",
    "        scale = (einops.reduce(residual.pow(2), \"batch position d_model -> batch position 1\", \"mean\") + cfg.layer_norm_eps).sqrt()\n",
    "        normalized = residual / scale\n",
    "        normalized = normalized * self.w + self.b\n",
    "        if self.cfg.debug: print(\"Normalized:\", residual.shape)\n",
    "        return normalized\n",
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacec595-90b1-4987-8338-93aeb450f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "Embeddings: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        \"YOUR CODE HERE\"\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        embed = self.W_E[tokens, :] # [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Embeddings:\", embed.shape)\n",
    "        return embed\n",
    "        \n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d3eecc-59cb-4118-8895-1fd876a32736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "           2.8267e-02,  5.4490e-02],\n",
       "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "           1.0172e-02, -1.5573e-04],\n",
       "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "           1.9325e-02, -2.1424e-02],\n",
       "         ...,\n",
       "         [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
       "          -2.3037e-03, -4.3189e-03],\n",
       "         [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
       "          -4.8160e-03, -9.2252e-04],\n",
       "         [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
       "          -3.2512e-03, -9.6509e-04]]], device='cuda:0',\n",
       "       grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        pos_embed = self.W_pos[:tokens.size(1), :] # [position, d_model]\n",
    "        pos_embed = einops.repeat(pos_embed, \"position d_model -> batch position d_model\", batch=tokens.size(0))\n",
    "        if self.cfg.debug: print(\"pos_embed:\", pos_embed.shape)\n",
    "        return pos_embed\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4be9b92-e3a0-46ce-bce7-2c3d599d4ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9663e-01,  1.6985e-02,  3.4781e-02,  ...,  3.3120e-02,\n",
       "          -2.3129e-02,  1.8103e-01],\n",
       "         [ 1.3162e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1997e-03,\n",
       "           5.3076e-03,  1.3511e-01],\n",
       "         [ 8.9738e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
       "           2.7959e-03,  9.0785e-02],\n",
       "         ...,\n",
       "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
       "          -4.9527e-04, -8.6008e-03],\n",
       "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6688e-02,\n",
       "          -1.4368e-02,  3.3245e-02],\n",
       "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
       "          -8.9028e-03,  4.8796e-02]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(0.0, dtype=torch.float32, device=\"cuda\"))\n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \"YOUR CODE HERE\"\n",
    "        q = einsum(self.W_Q, normalized_resid_pre, \"n_heads d_model d_head, batch position d_model -> batch position n_heads d_head\") + self.b_Q\n",
    "        k = einsum(self.W_K, normalized_resid_pre, \"n_heads d_model d_head, batch position d_model -> batch position n_heads d_head\") + self.b_K\n",
    "        v = einsum(self.W_V, normalized_resid_pre, \"n_heads d_model d_head, batch position d_model -> batch position n_heads d_head\") + self.b_V\n",
    "        qk = einsum(q, k, \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\")\n",
    "        attn_scores = qk / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "        pattern = attn_scores.softmax(dim=-1) # [batch, n_heads, query_pos, key_pos]\n",
    "        print(pattern.shape, v.shape)\n",
    "        z = einsum(pattern, v, \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\")\n",
    "        attn_out = einsum(z, self.W_O, \"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\") + self.b_O\n",
    "        return attn_out\n",
    "        \n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"blocks.0.ln1.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eab0865-5523-4e7b-b081-32364989f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
       "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
       "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
       "         ...,\n",
       "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
       "         [-0.0862,  0.7839,  0.9046,  ..., -0.2174, -0.5953,  0.8555],\n",
       "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "    \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_mid:\", normalized_resid_mid.shape)\n",
    "        pre = einsum(normalized_resid_mid, self.W_in, \"batch position d_model, d_model d_mlp -> batch position d_mlp\") + self.b_in\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = einsum(post, self.W_out, \"batch position d_mlp, d_mlp d_model -> batch position d_model\") + self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c5bd81-6d75-4d11-8461-b42fac2d4bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Normalized_resid_final: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Normalized_resid_final: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0878,  -54.3452,\n",
       "           -42.3645],\n",
       "         [-128.0392, -127.9936, -130.7010,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8819,  ..., -128.5180, -126.6027,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7750, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6725, -104.4889, -108.7361,  ..., -118.3552, -113.8767,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "    \n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_final:\", normalized_resid_final.shape)\n",
    "        logits = einsum(normalized_resid_final, self.W_U, \"batch position d_model, d_model d_vocab -> batch position d_vocab\") + self.b_U\n",
    "        return logits\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a22e4a4-4014-41fc-9e80-ce1c4f57bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOnly(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        \n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        resid_post = resid_mid + normalized_resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2199474b-a32f-4350-9485-31da1a9b15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3911,  0.1543,  0.6005,  ...,  1.7198,  1.7365,  0.3930],\n",
       "         [-0.9039, -0.0360,  0.2351,  ..., -0.4148,  0.3562,  0.3936],\n",
       "         [-0.9647, -2.4819, -1.4995,  ...,  1.4046,  0.7616,  0.5918],\n",
       "         ...,\n",
       "         [-0.7421,  0.9251, -0.3218,  ...,  0.2921,  0.1097, -0.5344],\n",
       "         [-1.3221,  0.8960,  1.1795,  ..., -0.5544, -0.4071,  0.9255],\n",
       "         [ 1.1209, -0.8919,  1.3737,  ..., -0.1356,  0.3434,  0.4517]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        \n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68b0e275-c2b5-496d-b4a5-bd5591b4768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Tokens: torch.Size([2, 4])\n",
      "Embeddings: torch.Size([2, 4, 768])\n",
      "Tokens: torch.Size([2, 4])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 12, 4, 4]) torch.Size([2, 4, 12, 64])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "Residual: torch.Size([2, 4, 768])\n",
      "Normalized: torch.Size([2, 4, 768])\n",
      "Normalized_resid_final: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Tokens: torch.Size([1, 35])\n",
      "Embeddings: torch.Size([1, 35, 768])\n",
      "Tokens: torch.Size([1, 35])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 12, 35, 35]) torch.Size([1, 35, 12, 64])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "Residual: torch.Size([1, 35, 768])\n",
      "Normalized: torch.Size([1, 35, 768])\n",
      "Normalized_resid_final: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0877,  -54.3452,\n",
       "           -42.3644],\n",
       "         [-128.0391, -127.9936, -130.7010,  ..., -136.7120, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8819,  ..., -128.5180, -126.6027,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7749, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6724, -104.4888, -108.7361,  ..., -118.3552, -113.8766,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        residual = embed + pos_embed\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        normalized_resid_final = self.ln_final(residual)\n",
    "        logits = self.unembed(normalized_resid_final)\n",
    "        # logits have shape [batch, position, logits]\n",
    "        return logits\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc7f68-fbe5-44e4-9fbd-b6ba15e2955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Safety",
   "language": "python",
   "name": "ai_safety"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
